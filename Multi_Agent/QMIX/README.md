# QMIX â€“ Cooperative Multi-Agent Reinforcement Learning

This directory contains an implementation of QMIX, a value-based multi-agent reinforcement learning algorithm designed for cooperative tasks with decentralized execution and centralized training.

QMIX is implemented following the formulation introduced in *QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning* (Rashid et al., 2018).

---

## Algorithm Overview

QMIX factorizes the global action-value function as:

Q_tot(s, a_1, ..., a_n) = f(Q_1, Q_2, ..., Q_n | s)

where:
- each agent learns a local action-value function Q_i based on its own observation,
- a mixing network combines the individual Q-values into a global Q_tot,
- the mixing network is constrained to be monotonic in each Q_i, ensuring that greedy local action selection is consistent with greedy global action selection.

The mixing network parameters are generated by hypernetworks conditioned on the global state.

---

## Motivation

This implementation is motivated by the limitations of additive value decomposition methods such as Value Decomposition Networks (VDN), which fail in tasks requiring non-linear coordination between agents.

QMIX extends value decomposition by allowing a more expressive, state-dependent combination of individual Q-values while preserving decentralized execution.

---

## Experiments

Preliminary experiments were conducted in simplified cooperative coordination environments, where:
- a global reward is obtained only when agents select coordinated joint actions,
- VDN fails due to its additive structure,
- QMIX successfully learns a coordinated policy.

These experiments serve as a validation of the implementation and reproduce the expected theoretical behavior of QMIX in comparison to VDN.

---

## Status

- QMIX with monotonic mixing network implemented from scratch
- Hypernetwork-based mixer validated on toy coordination tasks
- Current focus is on extending experiments and integrating additional components such as replay buffers and target networks

---

## References

- Rashid, T. et al. (2018). *QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning*.
- Russell, S., & Zimdars, A. (2003). *Q-Decomposition for Reinforcement Learning Agents*.